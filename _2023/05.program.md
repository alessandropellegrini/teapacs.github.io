---
layout: default
title: "Final Program"
menu_show: true
permalink: /:collection/program/
---

| 09:15 | Opening Remarks                  |
| 09:30 | Talk1: Giuliano Casale           |
| 10:15 | Talk2: Diwakar Krishnamurthy     |
| 11:00 | coffee break                     |
| 11:30 | FCRC Plenary                     |
| 12:30 | Lunch                            |
| 13:45 | Discussion Session (D1): What to teach|
| 14:45 | Talk3: Mohammad Hajiesmaili      |
| 15:30 | Coffee Break                     |
| 16:00 | Talk4: Ziv Scully                |
| 16:45 | Discussion Session (D2): How to teach |
| 17:45 | Closing Remarks                  |

## Speaker: Giuliano Casale
## Title: Performance evaluation teaching in the age of cloud computing

**Abstract**: Cloud computing has been a primary ICT driver during the last 15 years, fostering sharp changes in performance engineering practices across the computing industry and, at the same time, profoundly steering research trends in academia. A distinctive trait of this paradigm is that cloud engineers can programmatically control software and system performance, raising an expectation for computing graduates who find employment in this area to be ready to engage in hands-on performance engineering problems. This, in turn, calls for a broader and more profound education on performance topics as part of the computing curriculum. This talk will present my own experience and views on the intersection between cloud computing and performance evaluation teaching. I will also give practical examples of how we can engage students in the performance evaluation discipline and prepare them to tackle quality-of-service engineering challenges that arise in the cloud domain.

**Bio**: Giuliano Casale joined the Department of Computing at Imperial College London in 2010, where he is currently a Reader. He teaches and does research in performance engineering and cloud computing, topics on which he has published more than 150 refereed papers. He has served on the technical program committee of several conferences in performance and dependability. His research work has received multiple recognitions, including best paper awards at ACM SIGMETRICS, IEEE/IFIP DSN, and IEEE INFOCOM. He serves on the editorial boards of IEEE TNSM and ACM TOMPECS, as editor-in-chief of Elsevier PEVA, and as the current chair of ACM SIGMETRICS.

## Speaker: Diwakar Krishnamurthy
## Title: Teaching Software Performance Evaluation to Undergrads: Lessons Learned and Challenges

**Abstract**: Recent high profile performance-related outages and problems in industry clearly establish the importance of imparting performance evaluation skills to students at the undergrad level. Yet, performance engineering is rarely a required course in most software engineering programs around the world. The typical undergrad student is naturally drawn towards coding courses and courses on topics that they think are likely to be in demand in industry, e.g., AI and ML. While sympathetic, curriculum designers often cite student pressures and other factors such as accreditation requirements from engineering bodies to argue for not including a required performance evaluation course.

As a long time instructor of a required, undergrad software performance evaluation course, I will describe some of my experiences operating in such a climate. Specifically, I will outline key strategies I have followed to motivate students and overcome their resistance to the somewhat analytical nature of performance analysis. I will also offer my observations on how undergrad curriculums can be tuned to instil a performance-aware mindset into students. Finally, I will point out ongoing challenges and invite the audience to brainstorm some solutions.

**Bio**: Diwakar Krishnamurthy is a Professor at the Department of Electrical and Software Engineering at the University of Calgary in Calgary, Alberta, Canada. Diwakar’s research interests span all aspects of characterizing, testing, modeling, and optimizing the performance of software systems. Recently, his efforts have focused on automated performance management of cloud and big data systems. A key emphasis of his research is to devise practical performance engineering techniques that can be operationalized in industry contexts, He has collaborated very closely with industry partners such as HP, SAP, and Huawei towards this objective. Diwakar has served on the TPCs of many performance-themed conferences such as ACM SIGMETRICS and ACM/SPEC ICPE. He has won or has been nominated for many teaching awards, including winning one for teaching an undergrad performance evaluation course.

## Speaker: Mohammad Hajiesmaili
## Title: How does the learning-augmented design with joint classic and societal criteria revolutionize the foundations of performance analysis?
**Abstract**: Traditionally, computer systems are designed to optimize classic notions of performance such as flow completion time, cost, etc. The system performance is then typically evaluated by characterizing theoretical bounds in worst-case settings. For the next generation of computer systems, it is essential to elevate societal criteria, such as carbon awareness and equity, as first-class design goals. However, the classic performance metrics may conflict with societal criteria. Foundational understanding and performance evaluations of systems with these inherent tradeoffs lead to two categories of research questions that could be considered as possible new educational components for performance analysis courses. 
The classic techniques, e.g., worst-case analysis, for systems with conflicting objectives may lead to the impossibility of results. However, foundational understanding of the impossibility of results calls for new techniques and tools. In traditional performance evaluation, to understand the foundational limits, typically, it is sufficient to derive lower-bound arguments in worst-case settings. In the new era of system design, lower bounds are inherently about tradeoffs between different objectives. Characterizing these tradeoffs in settings with multiple design criteria is closer to the notion of  Pareto-optimality, which is drastically different from classic lower bounds. 
With the impossibility of results using classic paradigms, one possible direction is to (re)design systems following the emerging direction of learning-augmented algorithms. With this approach, it might be possible to remove/mitigate the foundational conflict between classic vs. societal metrics using the right predictions. However, the performance evaluation of learning-augmented algorithms calls for a new set of technical questions that we will explore a few of them in this talk. 

We will use a carbon-intelligent workload scheduling as a running example to explain the ideas.

**Bio**: Mohammad Hajiesmaili is an Assistant Professor with the Manning College of Information and Computer Sciences at the University of Massachusetts Amherst. His research focuses on developing optimization and machine learning tools to improve the energy and carbon efficiency of digital and cyberinfrastructure. He was named to Popular Science’s Brilliant 10 in 2022, featuring his work on the decarbonization of the internet. His awards and honors include an NSF CAREER Award, an Amazon Research Award, a Google Faculty Research Award, and other awards from NSF, VMWare, and Adobe. He has received five best paper runner-ups in the ACM e-Energy conference.

## Speaker: Ziv Scully 
to be decided

## Discussion Sessions
However, the success of the workshop depends on a productive dialogue and deliberation among all participants.  We, therefore, have two discussion sessions, during which the speakers will sit with other attendees and lead the discussion on two topics:

## (D1) What to teach
To keep up with the times, what new topics should be included in a performance modeling and analysis course, and what classical topics can be dropped?

What should the relationship be between such a course and (i) the standard curriculum for Computer Science and (ii) currently popular courses?

What do industry practitioners need to know regarding performance modeling and analysis?  Etc.

## (D2) How to teach
To suit students' academic preparation and learning habits, what innovation can instructors bring to design assignments, labs, projects and theses?

What has worked well, and what did not?  Etc.
